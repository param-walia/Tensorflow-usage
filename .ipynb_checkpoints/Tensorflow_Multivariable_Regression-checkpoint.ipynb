{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-feature Linear Regression with Tensorflow\n",
    "### The following code provides the basic skeleton for performing multivariable regression using Tensorflow and the code should be optimized for a real dataset. \n",
    "The code can also be seen as an introduction for the usage of tf.Variable, GradientDescentOptimizer, tf.Session etc. for a more complex classification or regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the data\n",
    "The following cell should be modified to input real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the data variables\n",
    "\n",
    "num_features = 4\n",
    "D = 30 #number of data points = length of the vectors\n",
    "\n",
    "X_data = np.zeros((D,num_features))\n",
    "for i in range(num_features):\n",
    "    X_data[:,i] = np.linspace(0,10,D) + np.random.uniform(-1,1,D)\n",
    "\n",
    "y_data = np.linspace(0,100,D) + np.random.uniform(-1,1,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the parameters\n",
    "With $y = m \\cdot x +b$, we have $D+1$ parameters: $m$ a D dimensional vector and $b$ a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.Variable(initial_value=tf.ones([num_features],tf.float64))\n",
    "b = tf.Variable(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = 0\n",
    "for X,y in zip(X_data,y_data):\n",
    "    y_hat = tf.tensordot(X,m,1) + tf.cast(b, tf.float64)\n",
    "    error += (y-y_hat)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate, should be adjusted for a larger range of features in the dataset\n",
    "lr = 1e-4 \n",
    "\n",
    "# relative difference in consecutive losses at which training should stop\n",
    "stopping = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tell Tensorflow what to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "train = optimizer.minimize(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow implementation:\n",
      "initial:\t m: [1. 1. 1. 1.]       \t b:1.000\t loss:34994.3659\n",
      "step:1000\t m: [2.075 2.593 1.798 3.44 ]\t b:0.822\t loss:336.1476\n",
      "step:2000\t m: [2.046 2.653 1.543 3.687]\t b:0.751\t loss:334.6226\n",
      "step:3000\t m: [2.049 2.666 1.462 3.758]\t b:0.736\t loss:334.4878\n",
      "\n",
      "Total no. of steps: 3974\n",
      "Final parameters m: [2.052 2.669 1.436 3.779]\t b:0.734\t loss:334.4752\n"
     ]
    }
   ],
   "source": [
    "max_steps =10000\n",
    "\n",
    "#the following command initializes the tf.Variables, for us m and b\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Training\n",
    "print('Tensorflow implementation:')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "  \n",
    "    print('initial:\\t m: {:20}\\t b:{:.3f}\\t loss:{:.4f}'\n",
    "          .format(str(sess.run(m)),sess.run(b),sess.run(error)))\n",
    "    \n",
    "    loss_last = sess.run(error) + 1.0\n",
    "\n",
    "    i = 0\n",
    "    loop = True\n",
    "    \n",
    "    while (loop & (i < max_steps)):\n",
    "        sess.run(train)\n",
    "        \n",
    "        if np.isnan(sess.run(error)):\n",
    "            print('Loss function became NAN, consider decreasing learning rate')\n",
    "            break\n",
    "        \n",
    "        #stopping criteria if the results are not changing\n",
    "        if ((np.abs(loss_last-sess.run(error))/loss_last < stopping)):\n",
    "            loop = False \n",
    "\n",
    "        i += 1\n",
    "        loss_last = sess.run(error)\n",
    "        \n",
    "        #Print some information\n",
    "        if ((i+1)%1000 ==0):\n",
    "            print('step:{:4d}\\t m: {:20}\\t b:{:.3f}\\t loss:{:.4f}'\n",
    "                  .format(i+1,str(sess.run(m)),sess.run(b),sess.run(error)))\n",
    "\n",
    "    print()\n",
    "    final_slope = sess.run(m)\n",
    "    final_intercept = sess.run(b)\n",
    "    if i < max_steps:\n",
    "        print('Total no. of steps: {:d}'.format(i))\n",
    "        print('Final parameters m: {:20}\\t b:{:.3f}\\t loss:{:.4f}'\n",
    "              .format(str(sess.run(m)),sess.run(b),sess.run(error)))\n",
    "    else:\n",
    "        print('Loss function did not converge according to the set criteria after {:d} steps'.format(max_steps))\n",
    "        print('Consider increasing max_steps or learning rate variable (lr)')      \n",
    "        print('Obtained parameters m: {:20}\\t b:{:.3f}\\t loss:{:.4f}'\n",
    "              .format(str(sess.run(m)),sess.run(b),sess.run(error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn implementation: m=[2.054 2.67  1.424 3.789]  \t b=0.733 \t loss=334.474\n"
     ]
    }
   ],
   "source": [
    "#Sanity check with Sklearn's function    \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X_data, y_data)\n",
    "sk_slope = reg.coef_\n",
    "sk_intercept = reg.intercept_\n",
    "\n",
    "print('Sklearn implementation: m={:5}  \\t b={:.3f} \\t loss={:.3f}'\n",
    "      .format(str(sk_slope),sk_intercept,\n",
    "              D*mean_squared_error(y_data, reg.predict(X_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the results\n",
    "if num_features == 1:\n",
    "    plt.plot(X_data,y_data,'+')\n",
    "    \n",
    "    #Tensorflow results\n",
    "    plt.plot(X_data,final_slope*X_data+final_intercept,'r')\n",
    "    \n",
    "    #Sklearn\n",
    "    plt.plot(X_data,sk_slope*X_data+sk_intercept,'b')\n",
    "    \n",
    "    plt.legend(('Data', 'Our Tensorflow implementation', 'from sklearn'),\n",
    "           loc='upper left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
