{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-feature Linear Regression with Tensorflow\n",
    "### The following code provides the basic skeleton for performing multivariable regression using Tensorflow.\n",
    "### The code should be optimized for a real dataset! \n",
    "The code can be seen as an introduction for the usage of tf.Variable, tf.placeholder, GradientDescentOptimizer, tf.Session etc. for a more complex classification or regression algorithm.\n",
    "\n",
    "I first implement a simple Regression for a small dataset and later I implement the batch feeding for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/param/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the data\n",
    "The following cell should be modified to input real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the data variables\n",
    "\n",
    "num_features = 4\n",
    "D = 30 #number of data points = length of the vectors\n",
    "\n",
    "X_data = np.zeros((D,num_features))\n",
    "for i in range(num_features):\n",
    "    X_data[:,i] = np.linspace(0,10,D) + np.random.uniform(-1,1,D)\n",
    "\n",
    "y_data = np.linspace(0,100,D) + np.random.uniform(-1,1,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the parameters\n",
    "With $y = m \\cdot x +b$, we have $D+1$ parameters: $m$ a D dimensional vector and $b$ a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.Variable(initial_value=tf.ones([num_features],tf.float64))\n",
    "b = tf.Variable(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = 0\n",
    "for X,y in zip(X_data,y_data):\n",
    "    y_hat = tf.tensordot(X,m,1) + tf.cast(b, tf.float64)\n",
    "    error += (y-y_hat)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate, should be adjusted for a larger range of features in the dataset\n",
    "lr = 1e-4 \n",
    "\n",
    "# relative difference in consecutive losses at which training should stop\n",
    "stopping = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tell Tensorflow what to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "train = optimizer.minimize(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow implementation:\n",
      "initial:\t m: [1. 1. 1. 1.]       \t b:1.000\t loss:34145.8193\n",
      "step:1000\t m: [2.767 2.912 2.793 1.287]\t b:0.458\t loss:226.9882\n",
      "step:2000\t m: [2.724 3.    2.89  1.182]\t b:0.212\t loss:225.9545\n",
      "step:3000\t m: [2.705 3.023 2.914 1.164]\t b:0.137\t loss:225.8721\n",
      "\n",
      "Total no. of steps: 3858\n",
      "Final parameters m: [2.7   3.029 2.92  1.16 ]\t b:0.117\t loss:225.8654\n"
     ]
    }
   ],
   "source": [
    "max_steps =10000\n",
    "\n",
    "#the following command initializes the tf.Variables, for us m and b\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Training\n",
    "print('Tensorflow implementation:')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "  \n",
    "    print('initial:\\t m: {:20}\\t b:{:.3f}\\t loss:{:.4f}'\n",
    "          .format(str(sess.run(m)),sess.run(b),sess.run(error)))\n",
    "    \n",
    "    loss_last = sess.run(error) + 1.0\n",
    "\n",
    "    i = 0\n",
    "    loop = True\n",
    "    \n",
    "    while (loop & (i < max_steps)):\n",
    "        sess.run(train)\n",
    "        \n",
    "        if np.isnan(sess.run(error)):\n",
    "            print('Loss function became NAN, consider decreasing learning rate')\n",
    "            break\n",
    "        \n",
    "        #stopping criteria if the results are not changing\n",
    "        if ((np.abs(loss_last-sess.run(error))/loss_last < stopping)):\n",
    "            loop = False \n",
    "\n",
    "        i += 1\n",
    "        loss_last = sess.run(error)\n",
    "        \n",
    "        #Print some information\n",
    "        if ((i+1)%(max_steps/10) ==0):\n",
    "            print('step:{:4d}\\t m: {:20}\\t b:{:.3f}\\t loss:{:.4f}'\n",
    "                  .format(i+1,str(sess.run(m)),sess.run(b),sess.run(error)))\n",
    "\n",
    "    print()\n",
    "    final_slope = sess.run(m)\n",
    "    final_intercept = sess.run(b)\n",
    "    if i < max_steps:\n",
    "        print('Total no. of steps: {:d}'.format(i))\n",
    "        print('Final parameters m: {:20}\\t b:{:.3f}\\t loss:{:.4f}'\n",
    "              .format(str(sess.run(m)),sess.run(b),sess.run(error)))\n",
    "    else:\n",
    "        print('Loss function did not converge according to the set criteria after {:d} steps'.format(max_steps))\n",
    "        print('Consider increasing max_steps or learning rate variable (lr)')      \n",
    "        print('Obtained parameters m: {:20}\\t b:{:.3f}\\t loss:{:.4f}'\n",
    "              .format(str(sess.run(m)),sess.run(b),sess.run(error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn implementation: m=[2.697 3.032 2.923 1.158]  \t b=0.107 \t loss=225.864\n"
     ]
    }
   ],
   "source": [
    "#Sanity check with Sklearn's function    \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X_data, y_data)\n",
    "sk_slope = reg.coef_\n",
    "sk_intercept = reg.intercept_\n",
    "\n",
    "print('Sklearn implementation: m={:5}  \\t b={:.3f} \\t loss={:.3f}'\n",
    "      .format(str(sk_slope),sk_intercept,\n",
    "              D*mean_squared_error(y_data, reg.predict(X_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the results\n",
    "if num_features == 1:\n",
    "    plt.plot(X_data,y_data,'+')\n",
    "    \n",
    "    #Tensorflow results\n",
    "    plt.plot(X_data,final_slope*X_data+final_intercept,'r')\n",
    "    \n",
    "    #Sklearn\n",
    "    plt.plot(X_data,sk_slope*X_data+sk_intercept,'b')\n",
    "    \n",
    "    plt.legend(('Data', 'Our Tensorflow implementation', 'from sklearn'),\n",
    "           loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the data variables\n",
    "\n",
    "num_features = 2\n",
    "D = 10000 #number of data points = length of the vectors\n",
    "\n",
    "X_data = np.zeros((D,num_features))\n",
    "for i in range(num_features):\n",
    "    X_data[:,i] = np.linspace(0,10,D) + np.random.uniform(-1,1,D)\n",
    "\n",
    "b0 = np.random.randint(3)\n",
    "y_data =  b0 + np.linspace(0,10,D) + np.random.uniform(-1,1,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.Variable(initial_value=tf.ones([num_features],tf.float64))\n",
    "b = tf.Variable(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "X_batch = tf.placeholder(tf.float64,shape=(batch_size,num_features),name='Xbatch')\n",
    "y_batch = tf.placeholder(tf.float64,shape=(batch_size),name='ybatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate, should be adjusted for a larger range of features in the dataset\n",
    "lr = 1e-4\n",
    "\n",
    "#Defining the optimizer\n",
    "y_hat = tf.tensordot(X_batch,m,1) + tf.cast(b, tf.float64)\n",
    "error = tf.reduce_sum(tf.square(y_batch-y_hat))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "train = optimizer.minimize(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:   1\t m: [0.93  0.927]       \t b:1.989\t  loss:38.4515\n",
      "epochs:1001\t m: [0.499 0.496]       \t b:1.991\t  loss:0.2562\n",
      "epochs:2001\t m: [0.5   0.498]       \t b:2.031\t  loss:0.6952\n",
      "epochs:3001\t m: [0.494 0.497]       \t b:2.058\t  loss:0.5215\n",
      "epochs:4001\t m: [0.496 0.496]       \t b:2.077\t  loss:0.3646\n",
      "epochs:5001\t m: [0.486 0.48 ]       \t b:2.074\t  loss:0.5245\n",
      "epochs:6001\t m: [0.493 0.502]       \t b:2.075\t  loss:0.7793\n",
      "epochs:7001\t m: [0.472 0.491]       \t b:2.074\t  loss:0.4562\n",
      "epochs:8001\t m: [0.473 0.498]       \t b:2.084\t  loss:0.4762\n",
      "epochs:9001\t m: [0.486 0.492]       \t b:2.084\t  loss:0.5188\n",
      "\n",
      "Final parameters m: [0.48  0.489]       \t b:2.085\t  loss:0.6732\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        rand_ind = np.random.randint(X_data.shape[0],size=batch_size)\n",
    "        feed = {X_batch:X_data[rand_ind,:],y_batch:y_data[rand_ind]}\n",
    "        \n",
    "        _,mstep,bstep,errorstep = sess.run([train,m,b,error],feed_dict=feed)\n",
    "        \n",
    "        if np.isnan(errorstep):\n",
    "            print('Loss function became NAN, consider decreasing learning rate')\n",
    "            break\n",
    "            \n",
    "        if ((i)%(epochs/10) ==0):\n",
    "            print('epochs:{:4d}\\t m: {:20}\\t b:{:.3f}\\t  loss:{:.4f}'\n",
    "                  .format(i+1,str(mstep),bstep,errorstep/batch_size))\n",
    "\n",
    "    print()\n",
    "    final_slope = mstep\n",
    "    final_intercept = bstep\n",
    "\n",
    "    print('Final parameters m: {:20}\\t b:{:.3f}\\t  loss:{:.4f}'.format(str(mstep),bstep,errorstep/batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn implementation: m=[0.491 0.491]  \t b=2.093 \t loss=0.497\n"
     ]
    }
   ],
   "source": [
    "#Sanity check with Sklearn's function    \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X_data, y_data)\n",
    "sk_slope = reg.coef_\n",
    "sk_intercept = reg.intercept_\n",
    "\n",
    "print('Sklearn implementation: m={:5}  \\t b={:.3f} \\t loss={:.3f}'\n",
    "      .format(str(sk_slope),sk_intercept,\n",
    "              mean_squared_error(y_data, reg.predict(X_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the results\n",
    "if num_features == 1:\n",
    "    rand_ind = np.random.randint(X_data.shape[0],size=100)\n",
    "    plt.plot(X_data[rand_ind,:],y_data[rand_ind],'+')\n",
    "    \n",
    "    #Tensorflow results\n",
    "    plt.plot(X_data,final_slope*X_data+final_intercept,'r')\n",
    "    \n",
    "    #Sklearn\n",
    "    plt.plot(X_data,sk_slope*X_data+sk_intercept,'b')\n",
    "    \n",
    "    plt.legend(('Data', 'Our Tensorflow implementation', 'from sklearn'),\n",
    "           loc='upper left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
