{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-feature Linear Regression with Tensorflow\n",
    "### The following code provides the basic skeleton for performing multivariable regression using Tensorflow.\n",
    "### The code should be optimized for a real dataset! \n",
    "The code can be seen as an introduction for the usage of tf.Variable, tf.placeholder, GradientDescentOptimizer, tf.Session etc. for a more complex classification or regression algorithm.\n",
    "\n",
    "I first implement a simple Regression for a small dataset.\n",
    "Then I implement the batch feeding for larger datasets.\n",
    "Lastly, I use the tf.estimator API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the data\n",
    "The following cell should be modified to input real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the data variables\n",
    "\n",
    "num_features = 4\n",
    "D = 30 #number of data points = length of the vectors\n",
    "\n",
    "X_data = np.zeros((D,num_features))\n",
    "for i in range(num_features):\n",
    "    X_data[:,i] = np.linspace(0,10,D) + np.random.uniform(-1,1,D)\n",
    "\n",
    "y_data = np.linspace(0,100,D) + np.random.uniform(-1,1,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the parameters\n",
    "With $y = m \\cdot x +b$, we have $D+1$ parameters: $m$ a D dimensional vector and $b$ a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.Variable(initial_value=tf.ones([num_features],tf.float64))\n",
    "b = tf.Variable(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = 0\n",
    "for X,y in zip(X_data,y_data):\n",
    "    y_hat = tf.tensordot(X,m,1) + tf.cast(b, tf.float64)\n",
    "    error += (y-y_hat)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate, should be adjusted for a larger range of features in the dataset\n",
    "lr = 1e-4 \n",
    "\n",
    "# relative difference in consecutive losses at which training should stop\n",
    "stopping = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tell Tensorflow what to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "train = optimizer.minimize(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow implementation:\n",
      "initial:\t m: [1. 1. 1. 1.]       \t b:1.000\t loss:34709.9886\n",
      "step:1000\t m: [3.575 2.187 2.026 1.944]\t b:1.548\t loss:306.0207\n",
      "step:2000\t m: [3.863 2.143 1.869 1.84 ]\t b:1.653\t loss:304.5272\n",
      "step:3000\t m: [3.939 2.136 1.814 1.819]\t b:1.688\t loss:304.4099\n",
      "\n",
      "Total no. of steps: 3910\n",
      "Final parameters m: [3.958 2.135 1.797 1.816]\t b:1.700\t loss:304.4000\n"
     ]
    }
   ],
   "source": [
    "max_steps =10000\n",
    "\n",
    "#the following command initializes the tf.Variables, for us m and b\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Training\n",
    "print('Tensorflow implementation:')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "  \n",
    "    print('initial:\\t m: {:20}\\t b:{:.3f}\\t loss:{:.4f}'\n",
    "          .format(str(sess.run(m)),sess.run(b),sess.run(error)))\n",
    "    \n",
    "    loss_last = sess.run(error) + 1.0\n",
    "\n",
    "    i = 0\n",
    "    loop = True\n",
    "    \n",
    "    while (loop & (i < max_steps)):\n",
    "        sess.run(train)\n",
    "        \n",
    "        if np.isnan(sess.run(error)):\n",
    "            print('Loss function became NAN, consider decreasing learning rate')\n",
    "            break\n",
    "        \n",
    "        #stopping criteria if the results are not changing\n",
    "        if ((np.abs(loss_last-sess.run(error))/loss_last < stopping)):\n",
    "            loop = False \n",
    "\n",
    "        i += 1\n",
    "        loss_last = sess.run(error)\n",
    "        \n",
    "        #Print some information\n",
    "        if ((i+1)%(max_steps/10) ==0):\n",
    "            print('step:{:4d}\\t m: {:20}\\t b:{:.3f}\\t loss:{:.4f}'\n",
    "                  .format(i+1,str(sess.run(m)),sess.run(b),sess.run(error)))\n",
    "\n",
    "    print()\n",
    "    final_slope = sess.run(m)\n",
    "    final_intercept = sess.run(b)\n",
    "    if i < max_steps:\n",
    "        print('Total no. of steps: {:d}'.format(i))\n",
    "        print('Final parameters m: {:20}\\t b:{:.3f}\\t loss:{:.4f}'\n",
    "              .format(str(sess.run(m)),sess.run(b),sess.run(error)))\n",
    "    else:\n",
    "        print('Loss function did not converge according to the set criteria after {:d} steps'.format(max_steps))\n",
    "        print('Consider increasing max_steps or learning rate variable (lr)')      \n",
    "        print('Obtained parameters m: {:20}\\t b:{:.3f}\\t loss:{:.4f}'\n",
    "              .format(str(sess.run(m)),sess.run(b),sess.run(error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn implementation: m=[3.967 2.135 1.787 1.816]  \t b=1.707 \t loss=304.399\n"
     ]
    }
   ],
   "source": [
    "#Sanity check with Sklearn's function    \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X_data, y_data)\n",
    "sk_slope = reg.coef_\n",
    "sk_intercept = reg.intercept_\n",
    "\n",
    "print('sklearn implementation: m={:5}  \\t b={:.3f} \\t loss={:.3f}'\n",
    "      .format(str(sk_slope),sk_intercept,\n",
    "              D*mean_squared_error(y_data, reg.predict(X_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the results\n",
    "if num_features == 1:\n",
    "    plt.plot(X_data,y_data,'+')\n",
    "    \n",
    "    #Tensorflow results\n",
    "    plt.plot(X_data,final_slope*X_data+final_intercept,'r')\n",
    "    \n",
    "    #Sklearn\n",
    "    plt.plot(X_data,sk_slope*X_data+sk_intercept,'b')\n",
    "    \n",
    "    plt.legend(('Data', 'Our Tensorflow implementation', 'from sklearn'),\n",
    "           loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the data variables\n",
    "\n",
    "num_features = 2\n",
    "D = 10000 #number of data points = length of the vectors\n",
    "\n",
    "X_data = np.zeros((D,num_features))\n",
    "for i in range(num_features):\n",
    "    X_data[:,i] = np.linspace(0,10,D) + np.random.uniform(-1,1,D)\n",
    "\n",
    "b0 = np.random.randint(3)\n",
    "y_data =  b0 + np.linspace(0,10,D) + np.random.uniform(-1,1,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.Variable(initial_value=tf.ones([num_features],tf.float64))\n",
    "b = tf.Variable(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "X_batch = tf.placeholder(tf.float64,shape=(batch_size,num_features),name='Xbatch')\n",
    "y_batch = tf.placeholder(tf.float64,shape=(batch_size),name='ybatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate, should be adjusted for a larger range of features in the dataset\n",
    "lr = 1e-4\n",
    "\n",
    "#Defining the optimizer\n",
    "y_hat = tf.tensordot(X_batch,m,1) + tf.cast(b, tf.float64)\n",
    "error = tf.reduce_sum(tf.square(y_batch-y_hat))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "train = optimizer.minimize(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:   1\t m: [0.866 0.869]       \t b:1.984\t  loss:66.5434\n",
      "epochs: 101\t m: [0.505 0.505]       \t b:1.930\t  loss:0.3689\n",
      "epochs: 201\t m: [0.501 0.5  ]       \t b:1.940\t  loss:0.8182\n",
      "epochs: 301\t m: [0.5   0.499]       \t b:1.950\t  loss:0.9044\n",
      "epochs: 401\t m: [0.517 0.515]       \t b:1.960\t  loss:0.3440\n",
      "epochs: 501\t m: [0.494 0.497]       \t b:1.963\t  loss:0.5353\n",
      "epochs: 601\t m: [0.504 0.505]       \t b:1.971\t  loss:0.4950\n",
      "epochs: 701\t m: [0.5   0.495]       \t b:1.975\t  loss:0.8292\n",
      "epochs: 801\t m: [0.502 0.497]       \t b:1.981\t  loss:0.3708\n",
      "epochs: 901\t m: [0.492 0.493]       \t b:1.988\t  loss:0.6541\n",
      "\n",
      "Final parameters m: [0.503 0.501]       \t b:1.994\t  loss:0.4100\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        rand_ind = np.random.randint(X_data.shape[0],size=batch_size)\n",
    "        feed = {X_batch:X_data[rand_ind,:],y_batch:y_data[rand_ind]}\n",
    "        \n",
    "        _,mstep,bstep,errorstep = sess.run([train,m,b,error],feed_dict=feed)\n",
    "        \n",
    "        if np.isnan(errorstep):\n",
    "            print('Loss function became NAN, consider decreasing learning rate')\n",
    "            break\n",
    "            \n",
    "        if ((i)%(epochs/10) ==0):\n",
    "            print('epochs:{:4d}\\t m: {:20}\\t b:{:.3f}\\t  loss:{:.4f}'\n",
    "                  .format(i+1,str(mstep),bstep,errorstep/batch_size))\n",
    "\n",
    "    print()\n",
    "    final_slope = mstep\n",
    "    final_intercept = bstep\n",
    "\n",
    "    print('Final parameters m: {:20}\\t b:{:.3f}\\t  loss:{:.4f}'.format(str(mstep),bstep,errorstep/batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn implementation: m=[0.504 0.475]  \t b=2.090 \t loss=0.498\n"
     ]
    }
   ],
   "source": [
    "#Sanity check with Sklearn's function    \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X_data, y_data)\n",
    "sk_slope = reg.coef_\n",
    "sk_intercept = reg.intercept_\n",
    "\n",
    "print('sklearn implementation: m={:5}  \\t b={:.3f} \\t loss={:.3f}'\n",
    "      .format(str(sk_slope),sk_intercept,\n",
    "              mean_squared_error(y_data, reg.predict(X_data))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tf.estimator API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is necessary to suppress Tensorflow logging\n",
    "#Comment this if you want to see the full output\n",
    "tf.logging.set_verbosity(tf.logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/ck/43m3bgl555n20p9b3hp8_f5m0000gp/T/tmpz77cyx0e\n"
     ]
    }
   ],
   "source": [
    "feat_cols = [tf.feature_column.numeric_column('x',shape=[num_features])]\n",
    "\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=feat_cols)\n",
    "\n",
    "input_func = tf.estimator.inputs.numpy_input_fn({'x':X_data},y_data,\n",
    "                                                batch_size=10,num_epochs=None,\n",
    "                                                shuffle=True)\n",
    "\n",
    "estimator.train(input_fn=input_func,steps =1000)\n",
    "train_metrics = estimator.evaluate(input_fn=input_func,steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average_loss': 0.50244284, 'label/mean': 7.016204, 'loss': 5.0244284, 'prediction/mean': 7.036784, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "print(train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['global_step',\n",
       " 'linear/linear_model/bias_weights',\n",
       " 'linear/linear_model/bias_weights/part_0/Ftrl',\n",
       " 'linear/linear_model/bias_weights/part_0/Ftrl_1',\n",
       " 'linear/linear_model/x/weights',\n",
       " 'linear/linear_model/x/weights/part_0/Ftrl',\n",
       " 'linear/linear_model/x/weights/part_0/Ftrl_1']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.get_variable_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_estimator = estimator.get_variable_value('linear/linear_model/x/weights')\n",
    "b_estimator = estimator.get_variable_value('linear/linear_model/bias_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From tf.estimator: m= [[0.503 0.496]] \t b= [2.012]\n"
     ]
    }
   ],
   "source": [
    "print('From tf.estimator: m=',m_estimator.T,'\\t b=',b_estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
